# ğŸ”¬ LangGraph Deep Research

> **Agentic research at production quality.** Drop in a topic, get back a fully cited, LLM-synthesised research report â€” streamed token-by-token in real time.

Built on **LangGraph** for stateful orchestration, **Parallel AI Deep Research** for comprehensive multi-source investigation, and **Novita AI (Qwen 3.5 397B)** for razor-sharp question generation and live streaming synthesis.

---

## âœ¨ What It Does

```
You: "impact of AI on financial markets"

  â†“ Qwen 3.5 397B generates 5 targeted research questions (~30 s)

  â†“ 5 Parallel AI ultra-processor tasks fire simultaneously (~20 min)
    â€¢ HFT algorithms and market volatility
    â€¢ NLP sentiment analysis and price discovery
    â€¢ Homogeneous AI models and flash-crash risk
    â€¢ ML credit scoring vs. logistic regression
    â€¢ Regulatory frameworks for black-box AI

  â†“ Qwen 3.5 397B synthesises a structured report â€” streamed live

You: A fully cited, multi-section research report with tables,
     event evidence, regulatory analysis, and a conclusion.
```

---

## ğŸ—ï¸ Architecture

### Workflow Graph

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  generate_questions â”‚  Novita AI Â· Qwen 3.5 397B
â”‚                     â”‚  â†’ 3â€“5 focused research questions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  conduct_research   â”‚  Parallel AI Â· ultra processor
â”‚                     â”‚  â†’ All questions fired simultaneously
â”‚   [Q1]â”€â”€[Q2]â”€â”€[Q3] â”‚    via ThreadPoolExecutor
â”‚      [Q4]â”€â”€[Q5]    â”‚  â†’ Results collected in original order
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  synthesize_results â”‚  Novita AI Â· Qwen 3.5 397B (stream=True)
â”‚                     â”‚  â†’ LLM-written report, streamed live
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
          END
```

### State Schema

```python
class ResearchState(TypedDict):
    topic: str                        # Original research topic
    questions: List[str]              # Generated research questions
    research_results: List[Dict]      # Parallel AI findings + citations
    final_report: Optional[str]       # LLM-synthesised markdown report
    step: str                         # Current workflow node
```

### Component Map

| Component | Technology | Responsibility |
|-----------|-----------|----------------|
| ğŸ§  **Question Generator** | Novita AI Â· Qwen 3.5 397B | Generates 3â€“5 sharp, non-overlapping research questions; strips `<think>` reasoning tokens before JSON parsing |
| ğŸ” **Research Client** | Parallel AI Â· ultra | Submits all questions in parallel; collects findings and citations; preserves order |
| ğŸ”€ **Orchestrator** | LangGraph | Manages `ResearchState`, node transitions, and error propagation |
| âœï¸ **Synthesiser** | Novita AI Â· Qwen 3.5 397B (streaming) | Writes the final report live via `stream=True`; falls back to template on failure |

---

## âš¡ Streaming Design

The `stream()` method yields a sequence of typed events so any consumer (CLI, API, WebSocket) can handle progress updates and report tokens independently:

```python
for event in workflow.stream("your topic"):

    if event["type"] == "status":
        # Progress update â€” e.g. "Generated 5 research questions"
        print(f"\n[{event['message']}]")
        if "questions" in event:
            for q in event["questions"]:
                print(f"  â€¢ {q}")

    elif event["type"] == "token":
        # Live synthesis token â€” print without newline for streaming effect
        print(event["content"], end="", flush=True)

    elif event["type"] == "complete":
        # Final state dict with the full report
        state = event["state"]
```

**Event types:**

| `type` | Extra keys | When |
|--------|-----------|------|
| `"status"` | `message`, optionally `questions` | Progress milestones |
| `"token"` | `content` | Each synthesis token from the LLM |
| `"complete"` | `state` | After the full report is assembled |

---

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.11+
- [Parallel AI](https://parallel.ai) API key (ultra processor)
- [Novita AI](https://novita.ai) API key

### 2. Install

```bash
cd langgraph_deep_research
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configure

```bash
cp .env.example .env
# add PARALLEL_API_KEY and NOVITA_API_KEY
```

Or use [Doppler](https://doppler.com) (recommended for teams):

```bash
doppler setup   # link to your Doppler project
doppler run -- python main.py "your topic"
```

---

## ğŸ–¥ï¸ CLI Usage

```bash
# Stream a full report (default)
python main.py "impact of AI on financial markets"

# Fast iteration â€” skip Parallel AI, use stub answers (~90 seconds total)
python main.py --mock "impact of AI on financial markets"

# Blocking mode â€” wait silently, then print the completed report
python main.py --no-stream "impact of AI on financial markets"

# Built-in help
python main.py --help
```

### `--mock` mode for fast iteration

The `--mock` flag is the key to tight development cycles:

| | `--mock` | Full run |
|---|---|---|
| Question generation (Novita AI) | âœ… Real | âœ… Real |
| Parallel AI deep research | âš¡ Instant stubs | âœ… Real |
| LLM synthesis streaming (Novita AI) | âœ… Real | âœ… Real |
| **Total time** | **~90 seconds** | **~20â€“30 minutes** |

Use `--mock` to iterate on prompts, streaming behaviour, and report formatting without burning Parallel AI credits or waiting 20 minutes per cycle.

---

## ğŸ“¦ Library Usage

### Streaming (recommended)

```python
from main import create_workflow

workflow = create_workflow()

for event in workflow.stream("quantum computing in cryptography"):
    if event["type"] == "token":
        print(event["content"], end="", flush=True)
    elif event["type"] == "status":
        print(f"\n[{event['message']}]")
```

### Blocking

```python
result = workflow.run("quantum computing in cryptography")
print(result["final_report"])
```

### Mock mode (fast iteration)

```python
for event in workflow.stream("quantum computing", mock=True):
    ...  # Same event types, Parallel AI replaced with instant stubs
```

### Individual tools

```python
from tools.question_generator import generate_research_questions
from tools.parallel_research import conduct_research_multiple

questions = generate_research_questions("AI ethics in healthcare")
results   = conduct_research_multiple(questions)   # runs in parallel
```

---

## ğŸ§ª Testing

```bash
python test_workflow.py
```

All 5 tests run with mocked APIs â€” **no API keys required**:

| Test | What it checks |
|------|---------------|
| `test_question_generation` | Fallback questions, state mutation |
| `test_research_execution` | Parallel client, error recovery |
| `test_result_synthesis` | LLM synthesis with template fallback |
| `test_state_transitions` | Full graph traversal via `workflow.run()` |
| `test_output_formatting` | Markdown structure, citation links |

---

## âš™ï¸ Configuration Reference

### Environment variables

| Variable | Required | Description |
|----------|----------|-------------|
| `PARALLEL_API_KEY` | âœ… | Parallel AI Deep Research key |
| `NOVITA_API_KEY` | âœ… | Novita AI key (Qwen 3.5 397B) |

### `config.py` tunables

```python
# Question generation
MIN_QUESTIONS = 3             # Lower bound on questions generated
MAX_QUESTIONS = 5             # Upper bound on questions generated

# Novita AI
NOVITA_TEMPERATURE       = 0.7   # Creativity (0.0 = deterministic)
NOVITA_MAX_TOKENS        = 2000  # Budget for question generation
NOVITA_SYNTHESIS_MAX_TOKENS = 8000  # Budget for report synthesis

# Parallel AI
PARALLEL_PROCESSOR_TYPE  = "ultra"  # Research quality tier
PARALLEL_TIMEOUT         = 300      # Per-task timeout (seconds)
```

> **Why separate token limits?** Question generation needs ~200â€“400 tokens. Report synthesis for 5 deeply researched questions easily exceeds 2 000 tokens â€” the original shared limit caused the last section to be cut off mid-sentence.

---

## ğŸ“ Project Structure

```
langgraph_deep_research/
â”œâ”€â”€ main.py                 # Workflow, streaming, CLI entry point
â”œâ”€â”€ config.py               # API keys and tunables
â”œâ”€â”€ example.py              # Runnable demo (streaming + blocking)
â”œâ”€â”€ test_workflow.py        # Full test suite (no API keys needed)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment template
â””â”€â”€ tools/
    â”œâ”€â”€ question_generator.py   # Novita AI Â· question generation
    â””â”€â”€ parallel_research.py    # Parallel AI Â· concurrent research
```

---

## ğŸ”§ Known Behaviours

| Behaviour | Explanation |
|-----------|-------------|
| **408 timeouts from Parallel AI** | Expected. The SDK uses long-polling with a ~10-minute window. 408 means "still working" â€” the SDK retries automatically. |
| **`<think>` tag stripping** | Qwen 3.5 is a reasoning model that emits chain-of-thought before the JSON. The parser strips those blocks before extracting questions. |
| **Questions arrive out of order** | Research runs in parallel; `Completed question 3/5` may log before `2/5`. Results are always returned in original question order. |
| **LLM synthesis fallback** | If the Novita streaming call fails, the synthesiser falls back to a template-based report automatically. |

---

## ğŸ› ï¸ Extending the Workflow

### Add a custom node

```python
def validate_research(state: ResearchState) -> ResearchState:
    """Flag low-confidence results before synthesis."""
    state["research_results"] = [
        r for r in state["research_results"]
        if r.get("status") == "completed"
    ]
    return state

workflow.add_node("validate", validate_research)
workflow.add_edge("conduct_research", "validate")
workflow.add_edge("validate", "synthesize_results")
```

### Add conditional routing

```python
def needs_more_research(state: ResearchState) -> str:
    completed = sum(1 for r in state["research_results"] if r.get("status") == "completed")
    return "conduct_research" if completed < 3 else "synthesize_results"

workflow.add_conditional_edges("conduct_research", needs_more_research)
```

### Swap the LLM

```python
# config.py
NOVITA_MODEL    = "meta-llama/llama-3.1-405b-instruct"
NOVITA_BASE_URL = "https://api.novita.ai/v3/openai"

# Or point at any OpenAI-compatible endpoint
NOVITA_BASE_URL = "https://api.openai.com/v1"
NOVITA_MODEL    = "gpt-4o"
```

---

## ğŸ“Š Performance & Cost

### Typical timing

| Stage | Time |
|-------|------|
| Question generation | 20â€“60 s |
| Deep research (5 Ã— ultra, parallel) | 15â€“30 min |
| LLM synthesis (streaming) | 1â€“3 min |
| **Total** | **~20â€“35 min** |

With `--mock`: **~90 seconds**.

### Approximate cost (5 questions)

| Service | Cost |
|---------|------|
| Novita AI â€” question gen + synthesis | ~$0.01 |
| Parallel AI â€” 5 Ã— ultra tasks | ~$0.50â€“$2.50 |
| **Total per run** | **~$0.51â€“$2.51** |

---

## ğŸ”’ Security

- API keys loaded exclusively from environment variables â€” never hard-coded
- `.env` listed in `.gitignore`
- All inputs validated before API calls
- API errors logged but not surfaced to callers in raw form

---

## ğŸ“„ License

[Apache 2.0](../LICENSE)
